{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.youtube.com/watch?v=9sHcLvVXsns&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 torchtext Field,  TabularDataset, BucketIterator (toy example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- step \n",
    "    - 1. specify how preprocessing should be done -> Fields\n",
    "    - 2. use dataset to load the data -> Tabular (JSON,csv,tsv)\n",
    "    - 3. Construct an iterator to do batch, padding -> BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T15:06:40.794323Z",
     "start_time": "2020-08-02T15:06:40.792097Z"
    }
   },
   "outputs": [],
   "source": [
    "from  torchtext.data import Field, TabularDataset, BucketIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class `torchtext.data.Field(sequential=True, use_vocab=True, init_token=None, eos_token=None, fix_length=None, dtype=torch.int64, preprocessing=None, postprocessing=None, lower=False, tokenize=None, tokenizer_language='en', include_lengths=False, batch_first=False, pad_token='<pad>', unk_token='<unk>', pad_first=False, truncate_first=False, stop_words=None, is_target=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T15:21:20.575691Z",
     "start_time": "2020-08-02T15:21:20.568311Z"
    }
   },
   "outputs": [],
   "source": [
    "# 여기서는 간단한 토크나이저 활용\n",
    "tokenizer = lambda x : x.split()\n",
    "\n",
    "# 어떤 식으로 전처리할지 \n",
    "quote = Field(sequential=True, use_vocab=True, tokenize=tokenizer, lower=True)\n",
    "score = Field(sequential=False, use_vocab=False)\n",
    "\n",
    "# score 는 감정분석에서 긍부정 레이블, 질의응답에서 정답 등에 해당\n",
    "# csv는 '컬럼이름' : (원하는 이름, 컬럼이름) 의 형태\n",
    "fields = {'quote' : ('q', quote), 'score' : ('s', score)}\n",
    "\n",
    "# 훈련, 테스트로 나누기\n",
    "trainData, testData = TabularDataset.splits(\n",
    "                                            path='',\n",
    "                                            train = 'train.json',\n",
    "                                            test='test.json',\n",
    "                                            # validation='val'\n",
    "                                            format='json',\n",
    "                                            fields=fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TabularDataset` fields\n",
    "\n",
    "- tuple(str, Field)]: If using a list, the format must be CSV or TSV, and the values of the list should be tuples of (name, field). The fields should be in the same order as the columns in the CSV or TSV file, while tuples of (name, None) represent columns that will be ignored.\n",
    "\n",
    "- If using a dict, the keys should be a subset of the JSON keys or CSV/TSV columns, and the values should be tuples of (name, field). Keys not present in the input dictionary are ignored. This allows the user to rename columns from their JSON/CSV/TSV key names and also enables selecting a subset of columns to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T15:22:28.115295Z",
     "start_time": "2020-08-02T15:22:28.111915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.example.Example at 0x7f31f91a85d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T15:32:41.555250Z",
     "start_time": "2020-08-02T15:32:41.548003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['q', 's'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T15:32:43.037549Z",
     "start_time": "2020-08-02T15:32:43.034826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([['you', 'must', 'own', 'everything', 'in', 'your', 'world.', 'there', 'is', 'no', 'one', 'else', 'to', 'blame.'], '1'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData[0].__dict__.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T15:48:05.319974Z",
     "start_time": "2020-08-02T15:48:05.317107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([['you', 'must', 'own', 'everything', 'in', 'your', 'world.', 'there', 'is', 'no', 'one', 'else', 'to', 'blame.'], '1'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData[0].__dict__.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T15:47:49.866898Z",
     "start_time": "2020-08-02T15:47:49.857295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[33, 27],\n",
      "        [19, 29],\n",
      "        [24,  7],\n",
      "        [14, 26],\n",
      "        [15, 18],\n",
      "        [34,  2],\n",
      "        [32, 25],\n",
      "        [31,  1],\n",
      "        [16,  1],\n",
      "        [20,  1],\n",
      "        [22,  1],\n",
      "        [12,  1],\n",
      "        [ 5,  1],\n",
      "        [ 8,  1]])\n",
      "tensor([1, 0])\n",
      "tensor([[10],\n",
      "        [21],\n",
      "        [ 4],\n",
      "        [ 3],\n",
      "        [ 6],\n",
      "        [11],\n",
      "        [17],\n",
      "        [ 4],\n",
      "        [ 3],\n",
      "        [30],\n",
      "        [28],\n",
      "        [ 5],\n",
      "        [13],\n",
      "        [ 2],\n",
      "        [ 9],\n",
      "        [23]])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# trainData vocab 만들기\n",
    "# Construct the Vocab object for this field from one or more datasets.\n",
    "quote.build_vocab(trainData, max_size=10000, min_freq=1)\n",
    "\n",
    "# batch와 padding 기능\n",
    "# 모두 같은 길이가 된다\n",
    "trainIterator, testIterator = BucketIterator.splits(\n",
    "    (trainData, testData),\n",
    "    batch_size=2\n",
    "    # device='cuda'\n",
    ")\n",
    "\n",
    "for batch in trainIterator:\n",
    "    print(batch.q)\n",
    "    print(batch.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padding으로 1이 들어갔다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T15:32:15.967536Z",
     "start_time": "2020-08-02T15:32:15.957913Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[33, 27],\n",
      "        [19, 29],\n",
      "        [24,  7],\n",
      "        [14, 26],\n",
      "        [15, 18],\n",
      "        [34,  2],\n",
      "        [32, 25],\n",
      "        [31,  1],\n",
      "        [16,  1],\n",
      "        [20,  1],\n",
      "        [22,  1],\n",
      "        [12,  1],\n",
      "        [ 5,  1],\n",
      "        [ 8,  1]])\n",
      "tensor([1, 0])\n",
      "tensor([[10],\n",
      "        [21],\n",
      "        [ 4],\n",
      "        [ 3],\n",
      "        [ 6],\n",
      "        [11],\n",
      "        [17],\n",
      "        [ 4],\n",
      "        [ 3],\n",
      "        [30],\n",
      "        [28],\n",
      "        [ 5],\n",
      "        [13],\n",
      "        [ 2],\n",
      "        [ 9],\n",
      "        [23]])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = lambda x : x.split()\n",
    "\n",
    "quote = Field(sequential=True, use_vocab=True, tokenize=tokenizer, lower=True)\n",
    "score = Field(sequential=False, use_vocab=False)\n",
    "\n",
    "fields = {'quote' : ('q', quote), 'score' : ('s', score)}\n",
    "\n",
    "trainData, testData = TabularDataset.splits(\n",
    "                                            path='',\n",
    "                                            train = 'train.csv',\n",
    "                                            test='test.csv',\n",
    "                                            # validation='val'\n",
    "                                            format='csv',\n",
    "                                            fields=fields)\n",
    "\n",
    "quote.build_vocab(trainData, max_size=10000, min_freq=1)\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "trainIterator, testIterator = BucketIterator.splits(\n",
    "    (trainData, testData),\n",
    "    batch_size=2,\n",
    "    device=device)\n",
    "\n",
    "for batch in trainIterator:\n",
    "    print(batch.q)\n",
    "    print(batch.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 torchtext.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchtext.datasets` 이용 -> Machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T05:34:26.540304Z",
     "start_time": "2020-08-03T05:34:26.356020Z"
    }
   },
   "outputs": [],
   "source": [
    "from  torchtext.data import Field, BucketIterator\n",
    "import spacy\n",
    "from torchtext.datasets import Multi30k # 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T05:55:56.249006Z",
     "start_time": "2020-08-03T05:55:54.959748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'What', \"'s\", 'your', 'name']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저\n",
    "spacyEng = spacy.load('en')\n",
    "spacyGer = spacy.load('de')\n",
    "\n",
    "def tokenizeEng(text):\n",
    "    return [tok.text for tok in spacyEng.tokenizer(text)]\n",
    "def tokenizeGer(text):\n",
    "    return [tok.text for tok in spacyGer.tokenizer(text)]\n",
    "\n",
    "tokenizeEng(\"hi What's your name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T05:42:24.011946Z",
     "start_time": "2020-08-03T05:42:20.969947Z"
    }
   },
   "outputs": [],
   "source": [
    "# 거의 다 동일\n",
    "english = Field(sequential=True, use_vocab=True, tokenize=tokenizeEng)\n",
    "german = Field(sequential=True, use_vocab=True, tokenize=tokenizeGer)\n",
    "\n",
    "trainData, validationData, testData = Multi30k.splits(exts=('.de','.en'),\n",
    "                                                     fields=(german, english))\n",
    "\n",
    "english.build_vocab(trainData, max_size=10000, min_freq=2)\n",
    "german.build_vocab(trainData, max_size=10000, min_freq=2)\n",
    "\n",
    "trainIterator, validationIterator, testIterator = BucketIterator.splits(\n",
    "    (trainData, validationData, testData),\n",
    "    batch_size=32,\n",
    "    device = 'cpu')\n",
    "\n",
    "for batch in trainIterator:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T05:42:57.849244Z",
     "start_time": "2020-08-03T05:42:57.843087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5627"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english.vocab.stoi['me']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T05:43:19.558125Z",
     "start_time": "2020-08-03T05:43:19.555159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english.vocab.itos[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 한국어 example\n",
    "- https://wikidocs.net/65348 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T05:37:38.631256Z",
     "start_time": "2020-08-04T05:37:33.457313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings_test.txt', <http.client.HTTPMessage at 0x7f34276d6490>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T05:37:45.376086Z",
     "start_time": "2020-08-04T05:37:45.162983Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDf = pd.read_table('ratings_train.txt')\n",
    "testDf = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T05:37:50.096755Z",
     "start_time": "2020-08-04T05:37:50.072396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T05:48:27.130650Z",
     "start_time": "2020-08-04T05:48:27.128575Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:11:21.615392Z",
     "start_time": "2020-08-04T06:11:21.594669Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Mecab()\n",
    "\n",
    "text = Field(sequential=True, use_vocab=True, \n",
    "             tokenize=tokenizer.morphs,\n",
    "             batch_first=True, fix_length=20)\n",
    "\n",
    "label = Field(sequential=False, use_vocab=False, is_target=True)\n",
    "\n",
    "fields = {'document' : ('doc', text), 'label' : ('y', label)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:11:21.860496Z",
     "start_time": "2020-08-04T06:11:21.854573Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:11:37.577765Z",
     "start_time": "2020-08-04T06:11:22.186669Z"
    }
   },
   "outputs": [],
   "source": [
    "trainData, testData = TabularDataset.splits(\n",
    "    path='', train='ratings_train.txt', test='ratings_test.txt',\n",
    "    format='tsv', fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:11:37.581622Z",
     "start_time": "2020-08-04T06:11:37.578862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc': ['아', '더', '빙', '.', '.', '진짜', '짜증', '나', '네요', '목소리'], 'y': '0'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(trainData[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 집합 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:12:29.367946Z",
     "start_time": "2020-08-04T06:12:28.833950Z"
    }
   },
   "outputs": [],
   "source": [
    "text.build_vocab(trainData, min_freq=10, max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:12:29.918588Z",
     "start_time": "2020-08-04T06:12:29.910808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002\n"
     ]
    }
   ],
   "source": [
    "print(len(text.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터로더 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:05:58.978359Z",
     "start_time": "2020-08-04T06:05:58.959917Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:17:50.743185Z",
     "start_time": "2020-08-04T06:17:50.740879Z"
    }
   },
   "outputs": [],
   "source": [
    "batchSize=5\n",
    "\n",
    "trainLoader = Iterator(trainData, batch_size=batchSize)\n",
    "testLoader = Iterator(testData, batch_size=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:17:53.099777Z",
     "start_time": "2020-08-04T06:17:52.944140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 5]\n",
      "\t[.doc]:[torch.LongTensor of size 5x20]\n",
      "\t[.y]:[torch.LongTensor of size 5]\n"
     ]
    }
   ],
   "source": [
    "for batch in trainLoader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:07:05.582450Z",
     "start_time": "2020-08-04T06:07:05.573721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 20])\n",
      "tensor([[   0, 1249,  165,   12,   31,  282,  510,  439,  362,    7,    0,    0,\n",
      "            0,    0,   63,   12,  661,    7, 7162, 1961],\n",
      "        [   0, 7041,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 207,    0, 6317,    0,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   0,    0,    0,   19, 9940,    0, 4097,   67,    0,  717,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [1432,  111, 5038,   25,    0,  219,   47,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1]])\n"
     ]
    }
   ],
   "source": [
    "print(batch.doc.shape)\n",
    "print(batch.doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:18:08.317924Z",
     "start_time": "2020-08-04T06:18:08.160215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 5]\n",
      "\t[.doc]:[torch.LongTensor of size 5x20]\n",
      "\t[.y]:[torch.LongTensor of size 5]\n"
     ]
    }
   ],
   "source": [
    "trainIterator, testIterator = BucketIterator.splits(\n",
    "    (trainData, testData),\n",
    "    batch_size=batchSize,\n",
    "    device = 'cpu')\n",
    "\n",
    "for batch in trainIterator:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:18:26.938086Z",
     "start_time": "2020-08-04T06:18:26.935674Z"
    }
   },
   "source": [
    "`Iterator, BucketIterator` 둘이 뭐가 다른건지 모르겠다\n",
    "    - 아마 후자는 (train, val, test) 를 한번에 다룰 수 있는 것 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}